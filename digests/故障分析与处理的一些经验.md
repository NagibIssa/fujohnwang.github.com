# 故障分析与处理的一些经验

## 一般性故障的定位

### 故障定位
* 网络：针对各个重要的服务器节点实时icmp监控，高延时或者不通做预警
* 数据库连接池：连接池连接数是否正常（与连接数的配置也有关系，是否配置合理）
* cdn连接：网络连通性，缓存是否失效
* nginx：日志分析，是否有刷连接，400+，500+问题的比例
* 缓存异常：缓存连接
* 消息：消费者出现异常，调用失效，导致积压；生产者线程数为0或者低于正常配置
* 应用日志：日志生产量超过正常范围或者低于一个范围阈值，做简单的预警
* 服务接口

### 性能定位
* tp99/tp90：性能未达标，根据监控定位代码，直接优化代码（减少数据库查询次数，减少查询缓存的次数等）
* CPU和内存：是否有死锁和大量爆内存操作，缓存超常
* 负载：复杂运算（比如频繁读写大文件，高频率查询数据库），定时数据库任务
* jvm：死锁和GC的情况
* 缓存：命中率低
* 硬盘空间：日志写满，需要有阈值报警

### 线上BUG
* 一致性问题：分布式事务与最终一致性
* 访问量/订单波动率：访问量/订单的波动率高于阈值，波动异常
* 服务接口：降级方案备份

## 针对线上故障的解决之道
### 监控与预警：
* 针对网络，对所有的节点都要有基本的ICMP监控；
* 针对数据库，都需要有基本的连通性测试，以及连接池，qps/tps，各种命中率的监控（比如缓存的命中很低，大部分做全表扫描）
* 针对nginx，需要对日志做状态分析，可以分钟级的对日志做监控，分析出每分钟各种状态的请求，从而分析出对于url是否异常，能快速感知线上的问题
* 针对缓存，需要连通性，连接数，命中率等监控
* 针对消息，是否有积压和死线程，从而定位生产者和消费者是否异常
* 针对日志，日志的生产量波动率以及对特殊字眼的监控分析
* 针对服务，dubbo整套服务治理方案
	
### 降级与响应：
* 手动降级：网络切换，数据库切换（一般作为自动降级方案）
* 自动降级：对数据做分级分析，分成可降级和不可降级数据，在应用中做控制。
    缓存和数据库主从自动切换
    可降级数据无法获取自动丢弃
    备份方案自动切换
    服务接口本地方案
	
### 回滚与切流量：
* 支持回滚：回滚应用，暂时保持线上正常
* 部分流量切回：切分流量上线，保证整个应用不受影响

## 一般故障排查流程：
* 线上应用已经打不开（非404错误）：网络入口=》nginx服务器
* 线上应用已经打不开（404错误）：应用服务器
* 线上应用能打开，但时好时坏 ：个别应用服务器挂掉
* 线上服务正常，但提交订单失败 ：这种情况很复杂，有以下情况：
    从网络开始排查：网络入口=》订单的服务器=》订单服务接口/订单数据库
    通过日志服务器查看是否有异常：日志服务=》异常定位
    tp99/90数据中的exception是否有大波动：性能监控数据=》定位异常
* 线上服务能打开，但是执行缓慢 ：这种最难分析，包括CPU和内存都有可能影响
	
## 线上系统解决方案
* 小组内指派监控和管理应用，比如有人专门负责日志的排查，有人负责网络的排查，达到快速解决问题 
* 通过短信和邮件报警形式把已经出现的问题快速通知给相关人员 
* 做好充分的降级和备案措施，能快速解决线上问题（5分钟~10分钟能做到发现到解决问题）或者说能最大程度的做到自动降级 